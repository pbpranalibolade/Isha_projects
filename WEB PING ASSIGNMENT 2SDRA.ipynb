{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['Title','Company','Location','Experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(class_='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems = results.find_all('article',class_='jobTuple bgWhite br4 mb-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_=[]\n",
    "company_=[]\n",
    "location_=[]\n",
    "exp_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_elems:\n",
    "    title=i.find('a',class_=\"title fw500 ellipsis\")\n",
    "    title_.append(title.get_text().replace(\"\\n\",\"\"))\n",
    "    print(title.text)\n",
    "    company=i.find('a',class_=\"subTitle ellipsis fleft\")\n",
    "    company_.append(company.get_text().replace(\"\\n\",\"\"))\n",
    "    print(company.text)\n",
    "    location=i.find('li',class_=\"fleft grey-text br2 placeHolderLi location\")    \n",
    "    loc=location.find('span',class_=\"ellipsis fleft fs12 lh16\")\n",
    "    location_.append(loc.get_text().replace(\"\\n\",\"\"))\n",
    "    print(loc.text)\n",
    "    exp=i.find('span',class_=\"ellipsis fleft fs12 lh16\")\n",
    "    exp_.append(exp.get_text().replace(\"\\n\",\"\"))\n",
    "    print(exp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.append({'Title':title.text,'Company':company.text,'Location':loc,'Experience':exp},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "naukri_data=pd.DataFrame({})\n",
    "naukri_data['Title']=title_[0:10]\n",
    "naukri_data['Compny']=company_[0:10]\n",
    "naukri_data['Location']=location_[0:10]\n",
    "naukri_data['Experience']=exp_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naukri_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Naukri data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.naukri.com/data-scientist-jobs-in-banglore?k=data%20scientist&l=banglore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['Title','Company','Location','Jobdetails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(class_='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems = results.find_all('article',class_='jobTuple bgWhite br4 mb-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_=[]\n",
    "company_=[]\n",
    "location_=[]\n",
    "details_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_elems:\n",
    "    title=i.find('a',class_=\"title fw500 ellipsis\")\n",
    "    title_.append(title.get_text().replace(\"\\n\",\"\"))\n",
    "    print(title.text)\n",
    "    company=i.find('a',class_=\"subTitle ellipsis fleft\")\n",
    "    company_.append(company.get_text().replace(\"\\n\",\"\"))\n",
    "    print(company.text)\n",
    "    location=i.find('li',class_=\"fleft grey-text br2 placeHolderLi location\")    \n",
    "    loc=location.find('span',class_=\"ellipsis fleft fs12 lh16\")\n",
    "    location_.append(loc.get_text().replace(\"\\n\",\"\"))\n",
    "    print(loc.text)\n",
    "    details=i.find('div',class_=\"job-description fs12 grey-text\")\n",
    "    details_.append(exp.get_text().replace(\"\\n\",\"\"))\n",
    "    print(details.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "naukri_data_with_details=pd.DataFrame({})\n",
    "naukri_data_with_details['Title']=title_[0:10]\n",
    "naukri_data_with_details['Compny']=company_[0:10]\n",
    "naukri_data_with_details['Location']=location_[0:10]\n",
    "naukri_data_with_details['Jobdetails']=details_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naukri_data_with_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.naukri data with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://www.naukri.com/data-scientist-jobs-in-banglore?k=data%20scientist&l=banglore&cityType=25.9.31&ctcFilter=3to6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(class_='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems = results.find_all('article',class_='jobTuple bgWhite br4 mb-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_=[]\n",
    "company_=[]\n",
    "location_=[]\n",
    "exp_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_elems:\n",
    "    title=i.find('a',class_=\"title fw500 ellipsis\")\n",
    "    title_.append(title.get_text().replace(\"\\n\",\"\"))\n",
    "    print(title.text)\n",
    "    company=i.find('a',class_=\"subTitle ellipsis fleft\")\n",
    "    company_.append(company.get_text().replace(\"\\n\",\"\"))\n",
    "    print(company.text)\n",
    "    location=i.find('li',class_=\"fleft grey-text br2 placeHolderLi location\")    \n",
    "    loc=location.find('span',class_=\"ellipsis fleft fs12 lh16\")\n",
    "    location_.append(loc.get_text().replace(\"\\n\",\"\"))\n",
    "    print(loc.text)\n",
    "    exp=i.find('span',class_=\"ellipsis fleft fs12 lh16\")\n",
    "    exp_.append(exp.get_text().replace(\"\\n\",\"\"))\n",
    "    print(exp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "naukri_data_filter=pd.DataFrame({})\n",
    "naukri_data_filter['Title']=title_[0:10]\n",
    "naukri_data_filter['Compny']=company_[0:10]\n",
    "naukri_data_filter['Location']=location_[0:10]\n",
    "naukri_data_filter['Experience']=exp_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naukri_data_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor data scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page1.page_source,'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find('article',class_='noPad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = results.find_all('ul',class_='jlGrid hover p-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_=[]\n",
    "rating_=[]\n",
    "jobage_=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_elems:\n",
    "    title=soup.find('a',class_=\"css-10l5u4p e1n63ojh0 jobLink\")\n",
    "    title_.append(title.get_text().replace(\"\\n\",\"\"))\n",
    "    print(title.text)\n",
    "    \n",
    "    rating=soup.find('div',class_=\"d-flex flex-column css-fbt9gv e1rrn5ka2\")\n",
    "    rating=company.find('span',class_=\"compactStars \")\n",
    "    rating_.append(rating.get_text().replace(\"\\n\",\"\"))\n",
    "    print(rating.text)\n",
    "    age=i.find('div',class_=\"d-flex align-items-end pl-std css-mi55ob\")\n",
    "    jobage_.append(exp.get_text().replace(\"\\n\",\"\"))\n",
    "    print(age.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "glassdoor_data_filter=pd.DataFrame({})\n",
    "glassdoor_data_filter['Company']=title_[0:10]\n",
    "glassdoor_data_filter['Rating']=rating_[0:10]\n",
    "glassdoor_data_filter['JobPosted']=jobage_[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glassdoor_data_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor Slary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page1.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find('div',class_='_1xHGtK _373qXS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_=[]\n",
    "discription_=[]\n",
    "price_=[]\n",
    "disc_=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand=soup.find_all('div',class_=\"_2WkVRV\")\n",
    "brand[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in brand:\n",
    "    brand_.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "brand_[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discription=soup.find_all('a',class_=\"IRpwTa\")\n",
    "discription[0:40] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discription:\n",
    "    discription_.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "discription_[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=soup.find_all('div',class_=\"_30jeq3\")\n",
    "price[0:40] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in price:\n",
    "    price_.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "price_[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc=soup.find_all('div',class_=\"_3Ay6Sb\")\n",
    "disc[0:40] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in disc:\n",
    "    disc_.append(i.get_text().replace(\"\\n\",\"\"))\n",
    "disc_[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as Wait\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")\n",
    "\n",
    "page1=requests.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")\n",
    "soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "brand_=[]\n",
    "discription_=[]\n",
    "price_=[]\n",
    "disc_=[]\n",
    "\n",
    "print(\"a\")\n",
    "\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "while True:\n",
    "    \n",
    "        brand_.append(soup.find(\"div\", {\"class\": \"_2WkVRV\"}).get_text().strip())\n",
    "        discription_.append(soup.find(\"a\", {\"class\":\"IRpwTa\"}).get_text().strip())\n",
    "        price_.append(soup.find(\"div\", {\"class\": \"_30jeq3\"}).get_text().strip())\n",
    "        disc_.append(soup.find(\"div\", {\"class\": \"_3Ay6Sb\"}).get_text().strip())\n",
    "        df = pd.DataFrame({\"Brand\": brand_, \"Discription\": discription_, \"Price:\": price_, \"Discount\": disc_})\n",
    "        \n",
    "        try:\n",
    "            element = driver.find_element_by_xpath('//a/span[text()=\"Next\"]').click()\n",
    "            actions = ActionChains(driver)\n",
    "            menu = driver.find_element(By.CSS_SELECTOR, \".nav\")\n",
    "            hidden_submenu = driver.find_element(By.CSS_SELECTOR, \".nav #submenu1\")\n",
    "            ActionChains(driver).move_to_element(menu).click(hidden_submenu).perform()\n",
    "            #actions.move_to_element(element).perform()\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "            if len(brand_) < 101:\n",
    "                print(\"No more pages left\")\n",
    "                break\n",
    "               \n",
    "        \n",
    " #df.to_csv(\"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing requests, BeautifulSoup, pandas, csv\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "#command to create a structure of csv file in which we will populate our scraped data\n",
    "with open('Opencodez_Articles.csv', mode='w') as csv_file:\n",
    "   fieldnames = ['Link', 'Title', 'Para', 'Author', 'Date']\n",
    "   writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "   writer.writeheader()\n",
    "#Creating an empty lists of variables\n",
    "article_link = []\n",
    "article_title = []\n",
    "article_para = []\n",
    "article_author = []\n",
    "article_date = []\n",
    "#Defining the opencodezscraping function\n",
    "def opencodezscraping(webpage, page_number):\n",
    "   next_page = webpage + str(page_number)\n",
    "   response= requests.get(str(next_page)\n",
    "   soup = BeautifulSoup(response.content,\"html.parser\")\n",
    "   soup_title= soup.findAll(\"h2\",{\"class\":\"title\"})\n",
    "   soup_para= soup.findAll(\"div\",{\"class\":\"post-content image-caption-format-1\"})\n",
    "   soup_date= soup.findAll(\"span\",{\"class\":\"thetime\"})\n",
    "   for x in range(len(soup_title)):\n",
    "      article_author.append(soup_para[x].a.text.strip())\n",
    "      article_date.append(soup_date[x].text.strip())\n",
    "      article_link.append(soup_title[x].a['href'])\n",
    "      article_title.append(soup_title[x].a['title'])\n",
    "      article_para.append(soup_para[x].p.text.strip())\n",
    "   #Generating the next page url\n",
    "   if page_number < 16:\n",
    "      page_number = page_number + 1\n",
    "      opencodezscraping(webpage, page_number)\n",
    "   #calling the function with relevant parameters\n",
    "   opencodezscraping('https://www.opencodez.com/page/', 0)\n",
    "   \n",
    "   #creating the data frame and populating its data into the csv file\n",
    "data = { 'Article_Link': article_link,\n",
    "'Article_Title':article_title, 'Article_Para':article_para, 'Article_Author':article_author, 'Article_Date':article_date}\n",
    "df = DataFrame(data, columns = ['Article_Link','Article_Title','Article_Para','Article_Author','Article_Date'])\n",
    "df.to_csv(r'C:\\Users\\**\\**\\OpenCodez_Articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page1.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sci(url):\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    start_page=0\n",
    "    end_page=4\n",
    "    urls=[]\n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    for page in range(start_page,end_page+1):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs=soup.find_all('div', attrs ={'class':'container'})\n",
    "        for job in jobs:\n",
    "            rating_=i.find('div',class_=\"_3LWZlK\")\n",
    "            rating.append(rating_.get_text().replace(\"\\n\",\"\"))\n",
    "            print(rating[0:100])\n",
    "            review=i.find('div',class_=\"_2-N8zT\")\n",
    "            review_summary.append(review.get_text().replace(\"\\n\",\"\"))\n",
    "            fullreview=i.find('div',class_=\"t-ZTKy\")\n",
    "            full_review.append(fullreview.get_text().replace(\"\\n\",\"\"))\n",
    "            nxt_button=driver.find_element_by_xpath(\"//button[@class='btn-next-prev btn-next']\")\n",
    "            if nxt_button.text=='Next':\n",
    "                nxt_button.click()\n",
    "                time.sleep(5)\n",
    "\n",
    "    job_df=pd.DataFrame({'Rating':rating,\n",
    "                         'review':review_summary,\n",
    "                         'full_review':full_review})\n",
    "    #job_df.to_csv('Data Scientist_ND_Monster', index = False)\n",
    "    print(job_df)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sneaker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1=requests.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page1.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sci(url):\n",
    "    \n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\\\mayraj\\\\Downloads\\\\Isha\\\\ss\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    start_page=0\n",
    "    end_page=4\n",
    "    urls=[]\n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    for page in range(start_page,end_page+1):\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs=soup.find_all('div', attrs ={'class':'_2B099V'})\n",
    "        for i in jobs:\n",
    "            rating_=i.find('div',class_=\"_2WkVRV\")\n",
    "            rating.append(rating_.get_text().replace(\"\\n\",\"\"))\n",
    "            print(rating[0:100])\n",
    "            review=i.find('a')\n",
    "            review_summary.append(review.get_text().replace(\"\\n\",\"\"))\n",
    "            fullreview=i.find('div',class_=\"_30jeq3\")\n",
    "            full_review.append(fullreview.get_text().replace(\"\\n\",\"\"))\n",
    "            nxt_button=driver.find_element_by_xpath(\"//button[@class='btn-next-prev btn-next']\")\n",
    "            if nxt_button.text=='Next':\n",
    "                nxt_button.click()\n",
    "                time.sleep(5)\n",
    "\n",
    "    job_df=pd.DataFrame({'Rating':rating,\n",
    "                         'review':review_summary,\n",
    "                         'full_review':full_review})\n",
    "    #job_df.to_csv('Data Scientist_ND_Monster', index = False)\n",
    "    print(job_df)\n",
    "\n",
    "# Calling Function\n",
    "data_sci(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
